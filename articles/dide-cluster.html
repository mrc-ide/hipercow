<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en-GB">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>The DIDE Cluster • hipercow</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><link href="../extra.css" rel="stylesheet">
<meta property="og:title" content="The DIDE Cluster">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">hipercow</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.5</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/hipercow.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>General</h6></li>
    <li><a class="dropdown-item" href="../articles/packages.html">Packages and provisioning</a></li>
    <li><a class="dropdown-item" href="../articles/parallel.html">Parallel Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/troubleshooting.html">Troubleshooting</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Details</h6></li>
    <li><a class="dropdown-item" href="../articles/details.html">Details</a></li>
    <li><a class="dropdown-item" href="../articles/environments.html">Environments</a></li>
    <li><a class="dropdown-item" href="../articles/stan.html">Using stan</a></li>
    <li><a class="dropdown-item" href="../articles/INLA.html">Using INLA on Windows</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Clusters</h6></li>
    <li><a class="dropdown-item" href="../articles/dide-cluster.html">The DIDE Cluster</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Advanced topics</h6></li>
    <li><a class="dropdown-item" href="../articles/workers.html">Workers</a></li>
    <li><a class="dropdown-item" href="../articles/administration.html">Administration</a></li>
    <li><a class="dropdown-item" href="../articles/migration.html">Migration from didehpc</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/mrc-ide/hipercow/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.png" class="logo" alt=""><h1>The DIDE Cluster</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mrc-ide/hipercow/blob/main/vignettes/dide-cluster.Rmd" class="external-link"><code>vignettes/dide-cluster.Rmd</code></a></small>
      <div class="d-none name"><code>dide-cluster.Rmd</code></div>
    </div>

    
    
<!-- Please edit the file in vignettes_src/ -->
<p>We support one cluster at present (the DIDE cluster), which supports
running jobs on Windows and Linux nodes. We may add support for other
clusters in the future. We particular want to allow submitting jobs to
the central ICT/RCS Linux cluster via PBS, and if there is a need for
our own pure Linux cluster (as opposed to our MS-HPC hybrid), we may try
and create our own SLURM-based system.</p>
<p>This vignette collects instructions specific to our DIDE cluster
running on either Windows or Linux; the commands are the same for
either, but there are inevitably some differences concerning
<em>paths</em> which we’ll describe below.</p>
<div class="section level2">
<h2 id="pre-requisites-">Pre-requisites:-<a class="anchor" aria-label="anchor" href="#pre-requisites-"></a>
</h2>
<p>In short, you will need:</p>
<ul>
<li>to know your DIDE username and password</li>
<li>to have been added as a DIDE cluster user</li>
<li>to be connected to the DIDE network, (probably using ZScaler)</li>
<li>to have as your working directory a network share that the cluster
can see</li>
</ul>
<p>These are all explained in detail below. Once these are all
satisfied, you can initialise <code>hipercow</code> using
<code><a href="../reference/hipercow_init.html">hipercow_init()</a></code> use <code><a href="../reference/dide_check.html">dide_check()</a></code> to verify
everything works, and <code><a href="../reference/hipercow_hello.html">hipercow_hello()</a></code> to submit a test
task.</p>
</div>
<div class="section level2">
<h2 id="authentication-with-dide">Authentication with DIDE<a class="anchor" aria-label="anchor" href="#authentication-with-dide"></a>
</h2>
<p>First run the <code><a href="../reference/dide_authenticate.html">dide_authenticate()</a></code> function which will
talk you through entering your credentials and checking that they work.
You only need to do this <em>once per machine, each time you change your
password</em>.</p>
<p>A typical interaction with this looks like:</p>
<pre><code>&gt; dide_authenticate()
I need to unlock the system keychain in order to load and save
your credentials.  This might differ from your DIDE password,
and will be the password you use to log in to this particular
machine
Keyring password:
🔑  OK

── Please enter your DIDE credentials ──────────────────────────
We need to know your DIDE username and password in order to log
you into the cluster. This will be shared across all projects
on this machine, with the username and password stored securely
in your system keychain. You will have to run this command
again on other computers

Your DIDE password may differ from your Imperial password, and
in some cases your username may also differ. If in doubt,
perhaps try logging in at https://mrcdata.dide.ic.ac.uk/hpc and
use the combination that works for you there.

DIDE username (default: rfitzjoh) &gt;
Password:
🔑  OK

I am going to try and log in with your password now, if this
fails we can always try again, as failure is just the first
step towards great success.
Excellent news! Everything seems to work!</code></pre>
<p>If you hit problems, try going to <a href="https://mrcdata.dide.ic.ac.uk/hpc" class="external-link uri">https://mrcdata.dide.ic.ac.uk/hpc</a> and trying a few
combinations until you remember what it should be. Once done update this
in your password manager (perhaps <a href="https://bitwarden.com" class="external-link">BitWarden</a>?) so you can find it easily
next time.</p>
<p>It is possible that you do not have access to the cluster, even if
your username is correct. Try logging onto <a href="https://mrcdata.dide.ic.ac.uk/hpc/index.php" class="external-link">the portal</a>; if it
reports that you don’t have access then please request access by
messaging Wes. If you don’t know your username and password, read
on.</p>
<div class="section level3">
<h3 id="about-our-usernames-and-passwords">About our usernames and passwords<a class="anchor" aria-label="anchor" href="#about-our-usernames-and-passwords"></a>
</h3>
<p>For historical reasons DIDE exists on its own domain (DIDE) separate
from Imperial’s domain (IC). This means that you may have a different
DIDE username to your Imperial username (though usually the same) and
you may have a different password (though most people set these to be
the same).</p>
<p>The overall situation is summarised in this infographic:</p>
<p><img src="passwords.png"></p>
<p>If you need to change your DIDE password you can do this from any
DIDE domain machine by pressing Ctrl-Alt-Delete and following the
prompts. If it has already expired, such that you can’t login, you need
to contact Chris or Paul.</p>
<p>We store credentials using the <a href="https://keyring.r-lib.org/" class="external-link">keyring</a> package. This saves your
username and password securely in your system keyring, which will be
unlocked on login for at least Windows and macOS. You will need to rerun
<code><a href="../reference/dide_authenticate.html">dide_authenticate()</a></code> whenever you change your DIDE
password.</p>
<p>Linux users may have to install some additional system packages (on
Ubuntu/Debian etc this is <code>libsecret-1-dev</code>,
<code>libssl-dev</code>, and <code>libsodium-dev</code>) or you will
have to enter your keychain password each time you use
<code>hipercow</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="networks">Networks<a class="anchor" aria-label="anchor" href="#networks"></a>
</h2>
<p>Ensure you have connected to the DIDE network by using ZScaler; see
<a href="https://www.imperial.ac.uk/admin-services/ict/self-service/connect-communicate/remote-access/unified-access/" class="external-link">instructions
from ICT</a>, or by being on a desktop PC or virtual machine within the
building that is plugged in by Ethernet (this is less common now). Note
that being connected to the WiFi in the building is <em>not enough</em>
- you will need ZScaler.</p>
</div>
<div class="section level2">
<h2 id="filesystems-and-paths">Filesystems and paths<a class="anchor" aria-label="anchor" href="#filesystems-and-paths"></a>
</h2>
<p>For anything to work with <code>hipercow</code> on the DIDE cluster,
your working directory must be on a network share. If you are not sure
if you are on a network share, then run</p>
<pre><code><span><span class="fu"><a href="https://rdrr.io/r/base/getwd.html" class="external-link">getwd</a></span><span class="op">(</span><span class="op">)</span></span></code></pre>
<p>Interpreting this depends on your platform (the machine you are
typing commands into):</p>
<ul>
<li>
<strong>Windows</strong>: A drive like <code>C:</code> or
<code>D:</code> will be local. You should recognise the drive letter as
one like <code>Q:</code> that was mapped by default as your home or
<code>M:</code> that you mapped yourself as a project share.</li>
<li>
<strong>macOS</strong>: The path will likely be below
<code>/Volumes</code> (but not one that corresponds to a USB stick of
course!)</li>
<li>
<strong>Linux</strong>: The path should start at one of the mount
points you have configured in <code>/etc/fstab</code> as a network
path.</li>
</ul>
<div class="section level3">
<h3 id="cluster-based-storage-and-home-directories">Cluster-based storage and home directories<a class="anchor" aria-label="anchor" href="#cluster-based-storage-and-home-directories"></a>
</h3>
<p>We <strong>strongly recommend</strong> that you use cluster-based
storage for any serious work. The servers hosting these shares start
with <code>wpia-hn</code>, and you should use those rather than your
home directory (servers called <code>qdrive</code>, or
<code>wpia-san04</code>), or the temp drive or other departmental
project shares (servers starting with <code>wpia-didef4</code> or
<code>projects</code>).</p>
<p>The advantages of the cluster-based shares are that they are larger
(so you will run out of disk space more slowly) and faster than the
others. If you launch many tasks at once that use your home share you
can get unexpected failures as the disk can’t keep up with the amount of
data being read and written. Don’t use your home directory (generally
<code>Q:</code> on Windows) for anything more intensive than casual
experimentation. We have seen many people with jobs that fail
mysteriously on their network home directory when launched in parallel,
and this is generally fixed by using the cluster share.</p>
<p>If you don’t know if you have access to a cluster-based share, talk
to your PI about if one exists for you. If you are a PI, talk to Chris
(and/or Wes) about getting one set up.</p>
</div>
<div class="section level3">
<h3 id="mapping-network-drives-on-your-computer">Mapping network drives on your computer<a class="anchor" aria-label="anchor" href="#mapping-network-drives-on-your-computer"></a>
</h3>
<p>For all operating systems, you will need to be connected to the DIDE
departmental domain. Options are:-</p>
<ul>
<li>From outside the department, using ZScaler see the <a href="https://www.imperial.ac.uk/admin-services/ict/self-service/connect-communicate/remote-access/unified-access/" class="external-link">ICT
documentation</a> for details.</li>
<li>From inside the department on the wifi, <em>you also need ZScaler
connected</em>.</li>
<li>From inside the department on the wired network. This is the fastest
and most stable network, giving the best experience if it is available
to you.</li>
</ul>
<p>Below, instructions for setting up depend on the sort of computer you
are typing commands into at the moment (not the cluster type). We’re
going to use the example of mapping your home directory up; as we said
earlier, this isn’t recommended for proper cluster running, but everyone
should have a home share, so we can use it as an example.</p>
<div class="section level4">
<h4 id="windows">Windows<a class="anchor" aria-label="anchor" href="#windows"></a>
</h4>
<p>If you are using a Windows machine in the DIDE domain, then your home
drive (Q:) and the temp drive (T:) will likely already be mapped for
you. You cannot have a fully-qualified network name as your <em>current
working directory</em>, so although Windows can load files from a
fully-qualified network name, the results are not always as you might
hope.</p>
<p>So if you need to map another drive, or if you’re not in the DIDE
domain, then:-</p>
<ul>
<li>If not on DIDE domain, fire up ZScaler (with your IC
credentials)</li>
<li>Open This PC, and under the Computer tab, find Map network
drive.</li>
<li>That drops down to reveal another Map network drive button.</li>
<li>Choose a drive letter - which is arbitrary, the conventions are
<code>Q:</code> for your home drive, <code>T:</code> for the temp drive.
Please avoid using <code>I:</code> as we use this for Hipercow
internally, and anything before <code>D:</code> is likely to clash with
your system.</li>
<li>Set the fully-qualified network share; for example
<code>\\qdrive.dide.ic.ac.uk\homes\bob</code>
</li>
<li>Tick “Connect using different credentials”, and “Reconnect at
sign-in”.</li>
<li>Click Finish, and then in the credentials window, choose “More
choices”, “Use a different account”.</li>
<li>Specify your username including the domain - <code>DIDE\bob</code>
for example, and your DIDE password.</li>
<li>Choose “Remember me” if you’re happy to, and then “OK” and the drive
should get mapped.</li>
</ul>
</div>
<div class="section level4">
<h4 id="macos">macOS<a class="anchor" aria-label="anchor" href="#macos"></a>
</h4>
<p>In Finder, go to <code>Go -&gt; Connect to Server...</code> or press
<code>Command-K</code>. In the address field write the name of the share
you want to connect to. If your DIDE username is <code>bob</code>,
you’ll write:</p>
<pre><code>smb://qdrive.dide.ic.ac.uk/homes/bob</code></pre>
<p>which will work only for Bob’s home share. You will get a message
saying “You are attempt to connect to the server”qdrive.dide.ic.ac.uk”
and you press <em>Connect</em> to continue.</p>
<p>After that, the credentials page, and you’ll select <em>Registered
User</em> (not guest), your username, for example <code>DIDE\bob</code>
and you’ll provide your DIDE password. You may like to tick “Remember
this password” if you’re happy to.</p>
<p>This directory will be mounted at <code>/Volumes/bob</code> - that
is, the last folder of the fully-qualified domain name you provide
becomes the name of the local mountpoint within <code>/Volumes</code>).
There may be a better way of doing this, and the connection will not be
re-established automatically so if anyone has a better way let us
know.</p>
</div>
<div class="section level4">
<h4 id="linux">Linux<a class="anchor" aria-label="anchor" href="#linux"></a>
</h4>
<p>This is our current understanding of the best way. Full instructions
are <a href="https://help.ubuntu.com/community/MountWindowsSharesPermanently" class="external-link">on
the Ubuntu community wiki</a>.</p>
<p>First, install <code>cifs-utils</code></p>
<pre><code>sudo apt-get install cifs-utils</code></pre>
<p>In your <code>/etc/fstab</code> file, add</p>
<pre><code>//qdrive.dide.ic.ac.uk/homes/&lt;dide-username&gt; &lt;home-mount-point&gt; cifs uid=&lt;local-userid&gt;,gid=&lt;local-groupid&gt;,credentials=/home/&lt;local-username&gt;/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8 0  0</code></pre>
<p>where:</p>
<ul>
<li>
<code>&lt;dide-username&gt;</code> is your DIDE username without the
<code>DIDE\</code> bit.</li>
<li>
<code>&lt;local-username&gt;</code> is your local username (i.e.,
<code>echo $USER</code>).</li>
<li>
<code>&lt;local-userid&gt;</code> is your local numeric user id
(i.e. <code>id -u $USER</code>)</li>
<li>
<code>&lt;local-groupid&gt;</code> is your local numeric group id
(i.e. <code>id -g $USER</code>)</li>
<li>
<code>&lt;home-mount-point&gt;</code> is where you want your DIDE
home directory mounted</li>
</ul>
<p><strong>please back this file up before editing</strong>.</p>
<p>This is an example from Rich - his local Linux user is
<code>rich</code>, and his DIDE username is <code>rfitzjoh</code>, and
the uid and gid were looked up as above:-</p>
<pre><code>//qdrive.dide.ic.ac.uk/homes/rfitzjoh /home/rich/net/home cifs uid=1000,gid=1000,credentials=/home/rich/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8 0  0</code></pre>
<p>The file <code>.smbcredentials</code> contains</p>
<pre><code>username=&lt;dide-username&gt;
password=&lt;dide-password&gt;</code></pre>
<p>and set this to be chmod 600 for a modicum of security, but be aware
your password is stored in plaintext.</p>
<p>This set up is clearly insecure. I believe if you omit the
credentials line you can have the system prompt you for a password
interactively, but I’m not sure how that works with automatic
mounting.</p>
<p>Finally, run</p>
<pre><code>sudo mount -a</code></pre>
<p>to mount all drives and with any luck it will all work and you don’t
have to do this until you get a new computer.</p>
<p>If you are on a laptop that will not regularly be connected to the
internal network, you might want to add the option <code>noauto</code>
to the above</p>
<pre><code>//qdrive.dide.ic.ac.uk/homes/rfitzjoh /home/rich/net/home cifs uid=1000,gid=1000,credentials=/home/rich/.smbcredentials,domain=DIDE,sec=ntlmssp,iocharset=utf8,noauto 0  0</code></pre>
<p>and then explicitly mount the drive as required with</p>
<pre><code>sudo mount ~/net/home</code></pre>
</div>
</div>
</div>
<div class="section level2">
<h2 id="initialisation">Initialisation<a class="anchor" aria-label="anchor" href="#initialisation"></a>
</h2>
<p>For most use, it will be sufficient to write this if, for example you
want to run jobs on Windows nodes.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mrc-ide/hipercow" class="external-link">hipercow</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/hipercow_init.html">hipercow_init</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; ✔ Initialised hipercow at '.' (/home/rfitzjoh/net/home/cluster/hipercow-vignette/hv-20250423-7d83f6872b6bb)</span></span>
<span><span class="co">#&gt; ℹ Next, call 'hipercow_configure()'</span></span>
<span><span class="fu"><a href="../reference/hipercow_configure.html">hipercow_configure</a></span><span class="op">(</span>driver <span class="op">=</span> <span class="st">"dide-windows"</span><span class="op">)</span></span>
<span><span class="co">#&gt; ✔ Configured hipercow to use 'dide-windows'</span></span></code></pre></div>
<p>Instead, or additionally, you could configure with
<code>dide-linux</code> to indicate you’d like to run jobs on Linux
nodes. Configuring both drivers is fine if for this project you may want
to launch on both platforms. If you configure both, you’ll have to be
specific about which you want to use for provisioning and task creation,
whereas if there’s only one driver, it will be used by default. So see
which workflow you prefer, whether both from the same root, or keeping
things separate. The examples in the vignettes assume only one driver is
configured, so you won’t see the <code>driver</code> argument explicitly
set.</p>
<p>One more thing: if you are only using a single driver, then you can
combine the <code>init</code> and <code>configure</code> into, for
example, <code>hipercow_init("dide-windows")</code>.</p>
<p>By default, we aim to automatically detect your shares, and we
believe this works on Windows, macOS and Linux. If you are running on a
network share and the configuration errors because it cannot work out
what share you are on, please let us know.</p>
</div>
<div class="section level2">
<h2 id="does-it-work">Does it work?<a class="anchor" aria-label="anchor" href="#does-it-work"></a>
</h2>
<p>All the above steps can be checked automatically:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/mrc-ide/hipercow" class="external-link">hipercow</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/hipercow_init.html">hipercow_init</a></span><span class="op">(</span>driver <span class="op">=</span> <span class="st">"dide-windows"</span><span class="op">)</span></span>
<span><span class="co">#&gt; ℹ hipercow already initialised at '.' (/home/rfitzjoh/net/home/cluster/hipercow-vignette/hv-20250423-7d83f6872b6bb)</span></span>
<span><span class="co">#&gt; ℹ Configuration for 'dide-windows' unchanged</span></span>
<span><span class="fu"><a href="../reference/dide_check.html">dide_check</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; ✔ Found DIDE credentials for 'rfitzjoh'</span></span>
<span><span class="co">#&gt; ✔ DIDE credentials are correct</span></span>
<span><span class="co">#&gt; ✔ Connection to private network working</span></span>
<span><span class="co">#&gt; ✔ Path looks like it is on a network share</span></span>
<span><span class="co">#&gt; ℹ Using '/home/rfitzjoh/net/home/cluster/hipercow-vignette/hv-20250423-7d83f6872b6bb'</span></span>
<span><span class="co">#&gt; ✔ 'hipercow' and 'hipercow.dide' versions agree (1.1.3)</span></span></code></pre></div>
<p>Try a test task:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/hipercow_hello.html">hipercow_hello</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; ✔ Found DIDE credentials for 'rfitzjoh'</span></span>
<span><span class="co">#&gt; ✔ DIDE credentials are correct</span></span>
<span><span class="co">#&gt; ✔ Connection to private network working</span></span>
<span><span class="co">#&gt; ✔ Path looks like it is on a network share</span></span>
<span><span class="co">#&gt; ℹ Using '/home/rfitzjoh/net/home/cluster/hipercow-vignette/hv-20250423-7d83f6872b6bb'</span></span>
<span><span class="co">#&gt; ✔ 'hipercow' and 'hipercow.dide' versions agree (1.1.3)</span></span>
<span><span class="co">#&gt; ✔ Submitted task 'f9e8fa83205e66ef9cd8bd7f3ff1e549' using 'dide-windows'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── hipercow 1.0.55 running at 'V:/cluster/hipercow-vignette/hv-20250423-7d83f687</span></span>
<span><span class="co">#&gt; ℹ library paths:</span></span>
<span><span class="co">#&gt; •</span></span>
<span><span class="co">#&gt; V:/cluster/hipercow-vignette/hv-20250423-7d83f6872b6bb/hipercow/lib/windows/4.4.2</span></span>
<span><span class="co">#&gt; • I:/bootstrap-windows/4.4.2</span></span>
<span><span class="co">#&gt; • C:/Program Files/R/R-4.4.2/library</span></span>
<span><span class="co">#&gt; ℹ id: f9e8fa83205e66ef9cd8bd7f3ff1e549</span></span>
<span><span class="co">#&gt; ℹ starting at: 2025-04-23 10:57:58.901961</span></span>
<span><span class="co">#&gt; ℹ Task type: expression</span></span>
<span><span class="co">#&gt; • Expression: { [...]</span></span>
<span><span class="co">#&gt; • Locals: moo</span></span>
<span><span class="co">#&gt; • Environment: default</span></span>
<span><span class="co">#&gt;   R_GC_MEM_GROW: 3</span></span>
<span><span class="co">#&gt;   CMDSTAN: I:/cmdstan/cmdstan-2.35.0</span></span>
<span><span class="co">#&gt;   CMDSTANR_USE_RTOOLS: TRUE</span></span>
<span><span class="co">#&gt; ───────────────────────────────────────────────────────────────── task logs ↓ ──</span></span>
<span><span class="co">#&gt;  -----</span></span>
<span><span class="co">#&gt; Moooooo!</span></span>
<span><span class="co">#&gt;  ------</span></span>
<span><span class="co">#&gt;     \   ^__^</span></span>
<span><span class="co">#&gt;      \  (oo)\ ________</span></span>
<span><span class="co">#&gt;         (__)\         )\ /\</span></span>
<span><span class="co">#&gt;              ||------w|</span></span>
<span><span class="co">#&gt;              ||      ||</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ───────────────────────────────────────────────────────────────── task logs ↑ ──</span></span>
<span><span class="co">#&gt; ✔ status: success</span></span>
<span><span class="co">#&gt; ℹ finishing at: 2025-04-23 10:57:58.901961 (elapsed: 0.5092 secs)</span></span>
<span><span class="co">#&gt; ✔ Successfully ran test task 'f9e8fa83205e66ef9cd8bd7f3ff1e549'</span></span></code></pre></div>
<p>See the overall configuration (we will often ask for this):</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/hipercow_configuration.html">hipercow_configuration</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── hipercow root at /home/rfitzjoh/net/home/cluster/hipercow-vignette/hv-20250423-7d83f6872b6bb ─────────</span></span>
<span><span class="co">#&gt; ✔ Working directory '.' within root</span></span>
<span><span class="co">#&gt; ℹ R version 4.4.2 on Linux (rfitzjoh@wpia-dide300)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Packages ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ℹ This is hipercow 1.1.3</span></span>
<span><span class="co">#&gt; ℹ Installed: hipercow.dide (1.1.3), conan2 (1.9.101), logwatch (0.1.1), rrq (0.7.22)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Environments ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── default</span></span>
<span><span class="co">#&gt; • packages: (none)</span></span>
<span><span class="co">#&gt; • sources: (none)</span></span>
<span><span class="co">#&gt; • globals: (none)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── empty</span></span>
<span><span class="co">#&gt; • packages: (none)</span></span>
<span><span class="co">#&gt; • sources: (none)</span></span>
<span><span class="co">#&gt; • globals: (none)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Drivers ──</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ✔ 1 driver configured ('dide-windows')</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── dide-windows</span></span>
<span><span class="co">#&gt; • cluster: wpia-hn</span></span>
<span><span class="co">#&gt; • shares: 1 configured:</span></span>
<span><span class="co">#&gt; → (local) /home/rfitzjoh/net/home =&gt; \\qdrive.dide.ic.ac.uk\homes\rfitzjoh =&gt; V: (remote)</span></span>
<span><span class="co">#&gt; • r_version: 4.4.2</span></span>
<span><span class="co">#&gt; • path_lib: hipercow/lib/windows/4.4.2</span></span>
<span><span class="co">#&gt; • platform: windows</span></span>
<span><span class="co">#&gt; • username: rfitzjoh</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="the-dide-cluster">The DIDE Cluster<a class="anchor" aria-label="anchor" href="#the-dide-cluster"></a>
</h2>
<p>We have one MS-HPC (think Azure) cluster called <code>wpia-hn</code>
hosted in South Kensington, built mainly with recent 32-core, 512Gb
nodes, and some older but still capable nodes. Most of these are running
Windows, but MS-HPC can also attach Linux nodes, which we have now begun
supporting. Please let us now how you get on.</p>
<div class="section level3">
<h3 id="default-environment-variables">Default environment variables<a class="anchor" aria-label="anchor" href="#default-environment-variables"></a>
</h3>
<pre><code><span><span class="co">#&gt;                  name                     value</span></span>
<span><span class="co">#&gt; 1             CMDSTAN I:/cmdstan/cmdstan-2.35.0</span></span>
<span><span class="co">#&gt; 2 CMDSTANR_USE_RTOOLS                      TRUE</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="the-nodes">The Nodes<a class="anchor" aria-label="anchor" href="#the-nodes"></a>
</h3>
<p>In the <code>hipercow</code> root we just made, we can have a look at
what the cluster is like with <code><a href="../reference/hipercow_cluster_info.html">hipercow_cluster_info()</a></code>. This
gives us information we can then use with
<code>hipercow_resources</code> to request particular resources. See
<code><a href="../articles/parallel.html">vignette("parallel")</a></code> for more information.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/hipercow_cluster_info.html">hipercow_cluster_info</a></span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; $resources</span></span>
<span><span class="co">#&gt; $resources$name</span></span>
<span><span class="co">#&gt; [1] "wpia-hn"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$node_os</span></span>
<span><span class="co">#&gt; [1] "windows"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$max_cores</span></span>
<span><span class="co">#&gt; [1] 32</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$max_ram</span></span>
<span><span class="co">#&gt; [1] 512</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$queues</span></span>
<span><span class="co">#&gt; [1] "AllNodes" "Testing" </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$default_queue</span></span>
<span><span class="co">#&gt; [1] "AllNodes"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$build_queue</span></span>
<span><span class="co">#&gt; [1] "BuildQueue"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$testing_queue</span></span>
<span><span class="co">#&gt; [1] "Testing"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$redis_url</span></span>
<span><span class="co">#&gt; [1] "wpia-hn.hpc.dide.ic.ac.uk"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $resources$nodes</span></span>
<span><span class="co">#&gt;  [1] "wpia-001" "wpia-002" "wpia-003" "wpia-004" "wpia-005" "wpia-006"</span></span>
<span><span class="co">#&gt;  [7] "wpia-007" "wpia-008" "wpia-009" "wpia-010" "wpia-011" "wpia-012"</span></span>
<span><span class="co">#&gt; [13] "wpia-013" "wpia-014" "wpia-015" "wpia-016" "wpia-017" "wpia-018"</span></span>
<span><span class="co">#&gt; [19] "wpia-019" "wpia-020" "wpia-021" "wpia-022" "wpia-023" "wpia-024"</span></span>
<span><span class="co">#&gt; [25] "wpia-025" "wpia-026" "wpia-027" "wpia-028" "wpia-029" "wpia-030"</span></span>
<span><span class="co">#&gt; [31] "wpia-031" "wpia-032" "wpia-033" "wpia-034" "wpia-035" "wpia-036"</span></span>
<span><span class="co">#&gt; [37] "wpia-037" "wpia-038" "wpia-039" "wpia-040" "wpia-043" "wpia-044"</span></span>
<span><span class="co">#&gt; [43] "wpia-045" "wpia-046" "wpia-047" "wpia-048" "wpia-051" "wpia-052"</span></span>
<span><span class="co">#&gt; [49] "wpia-053" "wpia-054" "wpia-055" "wpia-056" "wpia-057" "wpia-058"</span></span>
<span><span class="co">#&gt; [55] "wpia-059" "wpia-060" "wpia-061" "wpia-062" "wpia-063" "wpia-064"</span></span>
<span><span class="co">#&gt; [61] "wpia-065" "wpia-066" "wpia-067" "wpia-068" "wpia-069" "wpia-070"</span></span>
<span><span class="co">#&gt; [67] "wpia-071" "wpia-072" "wpia-073" "wpia-074" "wpia-075" "wpia-076"</span></span>
<span><span class="co">#&gt; [73] "wpia-077" "wpia-078" "wpia-079" "wpia-080" "wpia-081" "wpia-082"</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $r_versions</span></span>
<span><span class="co">#&gt; [1] '4.2.3' '4.3.0' '4.3.2' '4.3.3' '4.4.0' '4.4.1' '4.4.2' '4.4.3'</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $redis_url</span></span>
<span><span class="co">#&gt; [1] "wpia-hn.hpc.dide.ic.ac.uk"</span></span></code></pre></div>
<ul>
<li><p><em>Nodes:</em> At present we have around 70 nodes, numbered
<code>wpia-xxx</code> - there are gaps because some nodes are
experimental, or serving other purposes not on the cluster at the
moment.</p></li>
<li><p><em>Cores:</em> at present, the largest node on this cluster has
32 cores - in fact all of the nodes do. This will change as we grow the
cluster, but for now, all the nodes are similar. The 32-core nodes are
our favourite compromise at the moment between power usage,
computational speed, and density, because physical space is
limited.</p></li>
<li><p><em>Memory:</em> most of the nodes have 512Gb of RAM; only nodes
059-070 are slightly smaller with 384Gb. This will change; some larger
nodes may arrive, and we may try and beef up some older nodes to ensure
you have enough RAM per job.</p></li>
<li><p><em>Queues:</em> for Windows nodes, the unfortunate legacy
<code>AllNodes</code> queue is used; for Linux <code>LinuxNodes</code>.
There is an additional node currently reserved for <code>Training</code>
and development, and if you come to a <code>hipercow</code> workshop,
you will likely submit small jobs to this one. Each driver (Windows or
Linux) has a default queue set, so your jobs should run on appropriate
nodes without you doing anything. We may add different queues to subset
our nodes into different capabilities, in which case more may
appear.</p></li>
<li><p><em>R versions:</em> you can see a few R versions that are
supported on all cluster nodes, including the most recent, and a couple
of older versions. We generally try to support the current and previous
versions of R. It’s a good idea to keep up-to-date.</p></li>
<li><p><em>The redis URL</em> is not something you need to do anything
with for the moment.</p></li>
</ul>
</div>
<div class="section level3">
<h3 id="cluster-storage">Cluster Storage<a class="anchor" aria-label="anchor" href="#cluster-storage"></a>
</h3>
<p>The headnode, <code>wpia-hn.hpc.dide.ic.ac.uk</code> has storage
which is connected to all the cluster nodes via a fast InfiniBand
network. When the cluster nodes talk to that share, they will use the
infiniband without you having to do anything special.</p>
<p>The storage can also be mapped (or mounted) so you can see the same
files from your desktop or laptop, just at a standard network speed.
You’ll need access to some share on that drive - talk to your PI about
that - and then follow the instructions earlier in this vignette for how
to map it.</p>
<p>The important thing is that you should definitely use this storage if
possible for any cluster tasks doing a significant amount of file
reading or writing, and it’s highly recommended to use it for all work
using this cluster.</p>
<p>Also, if you refer to data on the cluster storage from within your
task, this is the one area where you’ll have to do something different
if you are targeting Windows cluster nodes, compared to Linux cluster
nodes. We’ve already talked about mapping, mounting, or referring to
DIDE shares from your local computer. Here we need to talk about
referring to them <em>within a cluster job</em>, which might be running
on Windows, or Linux.</p>
<div class="section level4">
<h4 id="cluster-storage-on-windows-compute-nodes">Cluster Storage on Windows Compute nodes<a class="anchor" aria-label="anchor" href="#cluster-storage-on-windows-compute-nodes"></a>
</h4>
<p>You can refer to DIDE network shares using fully-qualified network
paths, so if I have a file in my home directory I want to read in R on a
Windows node, I can write:-</p>
<pre><code><span><span class="va">x</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/readLines.html" class="external-link">readLines</a></span><span class="op">(</span><span class="st">"//qdrive.dide.ic.ac.uk/homes/wes/hello.txt"</span><span class="op">)</span></span></code></pre>
<p>which is the same as I would write on a local Windows computer. This
will work for small tasks, but as we’ve said, you should not use this
for any large volumes of I/O. Instead you should you a share on the
cluster-storage.</p>
<p>Furthermore, if you are referring to the cluster-storage within a
cluster job, you should refer to <code>wpia-hn</code> as
<code>wpia-hn-app</code> to ensure you use the fast Infiniband network
connection to get your data. For example, if I have some data that I
refer to from my Windows local computer as
<code>//wpia-hn.hpc.dide.ic.ac.uk/flu-project/data.dat</code>, then to
access the same data from a cluster job, I should write
<code>//wpia-hn-app/flu-project/data.dat</code>.</p>
</div>
<div class="section level4">
<h4 id="cluster-storage-on-linux-compute-nodes">Cluster Storage on Linux Compute nodes<a class="anchor" aria-label="anchor" href="#cluster-storage-on-linux-compute-nodes"></a>
</h4>
<p>We cannot access network paths directly; they need to be explicitly
mounted, but that is hard to do reliably from within a cluster job. We
have therefore mounted the main cluster shares on all cluster nodes in
advance, and you will have the same read/write access as you would from
any other machine. But you will need to refer to the network shares in
the right way. Specifically:-</p>
<ul>
<li><p>To refer to a file on a <em>home directory</em> (for example,
<code>//qdrive.dide.ic.ac.uk/homes/wes/hello.txt</code>) from within a
cluster job on a Linux node, you should write
<code>/didehomes/wes/hello.txt</code> - noting yet again: we <em>do
not</em> recommend using home directories for any significant data
tasks.</p></li>
<li><p>To refer to data on a <em>cluster share</em> (for example,
<code>//wpia-hn.hpc.dide.ic.ac.uk/flu-project/data.dat</code>) from your
Linux cluster job, you should write
<code>/wpia-hn/flu-project/data.dat</code>. This is already wired up to
use the fast network connection.</p></li>
</ul>
<p>If you cannot find the data you are expecting, get in touch with Wes;
data has been moved around partly to prepare for Linux, and partly
because of our departmental relocation, so some data appearing dormant
has not yet been migrated.</p>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Rich FitzJohn, Wes Hinsley, Paul Liétar.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.2.</p>
</div>

    </footer>
</div>





  </body>
</html>
