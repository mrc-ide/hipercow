---
title: "Workers"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Workers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

<!-- This vignette really requires a real cluster behind it to work
     properly; we're going to submit a bunch of workers and tasks at
     them, and the example driver cannot support this without
     involving something better for the queue (otherwise it's very
     racy and we can't get multiple runners going.  We'll do the first
     bit of the PR with a single runner using 'example', then switch
     to the windows runner for some more exciting work. -->

```{r setup, include = FALSE}
source("../vignettes/common.R")
vignette_root <- new_hipercow_root_path(TRUE)
set_vignette_root(vignette_root)
```

```{r, echo = FALSE, results = "asis"}
add_header()
```

This vignette describes a pair of relatively advanced techniques for using a *second* queue embedded on top of the usual HPC scheduler in order to scale work; either running many more tasks than is convenient to run with the HPC scheduler, or running tasks that need more resources than you can easily get on a single cluster node.

If you have thousands and thousands of tasks to submit at once you may not want to flood the cluster with them all at once. Each task submission tends to be relatively slow on every platform that we have used, and submitting thousands of tasks will take minutes (even if you could submit 10 tasks a second, a thousand tasks will take almost two minutes, and a million tasks would take 27 hours!). Some cluster schedulers also slow down as the queue size increases, becoming less efficient at distributing work. And if you take up the whole cluster someone may come and find you in order to complain.  At the same time, batching your tasks up into little bits and manually sending them off is a pain and work better done by a computer.  An alternative is to submit a (relatively small) set of "workers" to the cluster, and then submit tasks to them.  We call this the **"lightweight queue pattern"**.

The second use case is where you want to run some computation on the cluster that needs to run some of its calculations in parallel, as you might do following instructions in `vignette("parallel")`.  Suppose that your cluster only has nodes with 32 cores though, and you have some calculations that would benefit from more cores than this.  Or suppose that you want to farm out some smaller number number of sub-tasks each of which will consume an entire node.  You can use the approach described here to scale your tasks off a single node, and we call this the **"interprocess communication pattern"**.

Both of these patterns are enabled with our [`rrq`](https://mrc-ide.github.io/rrq) package, along with a [Redis](https://redis.io) server which is running on the cluster.

# Getting started

To get started, you will need the `rrq` package, if you do not have it already (this will be installed automatically by hipercow, you can skip this step if you want)

```r
install.packages(
  "rrq",
  repos = c("https://mrc-ide.r-universe.dev", "https://cloud.r-project.org"))
```

You'll want a very recent version; here we are using version `r as.character(packageVersion("rrq"))`

```{r}
library(hipercow)
hipercow_init(driver = "example")
```

```{r, include = FALSE}
cleanup <- withr::with_dir(
  vignette_root,
  hipercow::hipercow_example_helper(with_logging = TRUE,
                                    new_directory = FALSE,
                                    initialise = FALSE))
```

# The lightweight queue pattern

Most interaction with `rrq` is done via a "controller"; this is an object that you can use to submit tasks and query on their status:

```{r}
r <- hipercow_rrq_controller()
```

The interface to this controller is subject to change, but many of the ideas will feel very similar to you from hipercow itself.  The controller object is just a handle that can be used with most functions in the rrq package:

```{r}
r
```

The other thing you'll need are some workers.  Let's submit a single worker to the cluster, and wait for it to become available:

```{r}
info <- hipercow_rrq_workers_submit(1)
info
```

TODO:

* check on the job status while waiting for workers - this should be easy to do
* rrq not installed in the bootstrap, but it should be

The workers are submitted as task bundles and can be inspected using their bundle name like any other task:

```{r}
hipercow_bundle_status(info$bundle_name)
```

This worker will remain running for 10 minutes after the last piece of work it runs.

## Basic usage

We'll load the `rrq` packages to make the calls a little clearer to read:

```{r}
library(rrq)
```

Submitting a task works much the same as hipercow, except that rather than `task_create_expr` you will use `rrq_task_create_expr` and pass the the controller as an argument:

```{r}
id <- rrq_task_create_expr(runif(10), controller = r)
```

as with hipercow, this `id` is a hex string:

```{r}
id
```

There's nothing here to distinguish this from a task identifier in hipercow itself (both just use strings), so be careful with your workflows.

Once you have you have your task, interacting with it will feel familiar as you can query its status, wait on it and fetch the result:

```{r}
rrq_task_status(id, controller = r)
rrq_task_wait(id, controller = r)
rrq_task_result(id, controller = r)
```

The big difference here from hipercow is how fast this process should be; the roundtrip of a task here will be a (hopefully small) fraction of a second:

```{r}
system.time({
  id <- rrq_task_create_expr(runif(10), controller = r)
  rrq_task_wait(id, controller = r)
  rrq_task_result(id, controller = r)
})
```

## Scaling up

Let's submit 1,000 trivial tasks, using `rrq_task_create_bulk_expr`, taking the square root of the first thousand positive integers.

```{r, include = FALSE}
t0 <- Sys.time()
```
```{r}
ids <- rrq_task_create_bulk_expr(sqrt(x), data.frame(x = 1:1000))
```

There's no equivalent of a task bundle in `rrq`; this just returns a vector of task 1000 task identifiers.  You can pass this vector in to `rrq_task_wait()` though, and then fetch the results using `rrq_task_results()` (note the pluralisation; `rrq_task_results()` always returns a list, while `rrq_task_result()` fetches a single task result).

```{r}
ok <- rrq_task_wait(ids)
result <- rrq_task_results(ids)
```

```{r, include = FALSE}
t1 <- Sys.time()
elapsed <- as.numeric(t1 - t0, "secs")
```

This process has taken `r round(elapsed, 2)` seconds, which is likely much faster than submitting this many tasks directly.

```{r, include = FALSE}
rrq_destroy()
cleanup()
```

# Interprocess commuication pattern

You can submit a task to the cluster that accesses a pool of workers.  This is quite difficult to demonstrate in a clear way, but bear with us.

The general pattern we try to achieve here is this:

1. Submit a normal hipercow task to the cluster
2. This task distributes work over a set of workers; this might happen several times
3. Return some summary of this work from your main task

A practical example of this approach might be to submit a hipercow task that runs an orderly report and within that you run an MCMC where you send each chain to a different worker (which may each use multiple cores).  This gives you three levels of parallelism and the ability to span past a single node fairly easily.  Though you may need to use a pen and paper to keep track of what you're doing.

For this example, we'll use the "windows" driver and submit work to a series of four workers running on the DIDE windows cluster.

```{r}
hipercow_init(driver = "windows")
r <- hipercow_rrq_controller()
```

```{r, include = FALSE}
vignette_root <- new_hipercow_root_path(TRUE)
set_vignette_root(vignette_root)
code <- c(
  "example <- function(n) {",
  "  ids <- rrq:rrq_task_create_bulk_call(sqrt, seq_len(n))",
  "  ok <- rrq::rrq_task_wait(ids)",
  "  stopifnot(ok)",
  "  result <- rrq::rrq_task_results(ids)",
  "  rrq::rrq_task_delete(ids)",
  "  sum(unlist(result))",
  "}")
writeLines(code, "code.R")
```

We'll submit a hipercow task that runs this small piece of code:

```{r, echo = FALSE, results = "asis"}
r_output(readLines("code.R"))
```

Hopefully this is fairly self-explanatory, if a bit pointless.  Note that we delete the rrq tasks after completing them, which prevents rrq tasks accumulating.

This code needs to be available in the hipercow environment run by the task:

```{r}
hipercow_environment_create("default", sources = "code.R")
```

We also need some workers - let's submit 4 of them:

```{r}
info <- hipercow_rrq_workers_submit(4)
```

now submit a task that will call this function:

```{r}
id <- task_create_expr(
  example(16),
  parallel = hipercow_parallel(use_rrq = TRUE))
id
```

Here, `id` is a *hipercow* task identifier.  This sends an additional task to the queue, so now we have five things running on the cluster (four workers and one task).  The task runs, picks up the controller, then uses it do distribute a (trivial) calculation over the four workers.

```{r}
task_wait(id)
task_result(id)
```

You can see from the worker logs here the tasks being split between the workers:

```{r}
rrq_worker_log_tail(n = 32)
```

This example is trivial, but you could submit 10 workers each using a 32 core node, and then use a single core task to farm out a series of large simulations across your bank of computers.  Or create a 500 single core workers (so ~25% of the cluster) and smash through a huge number of simulations with minimal overhead.

# Other points

## Stopping redundant workers

By default, workers will live for 10 minutes after finishing their last task.  This means that most of the time that you use workers you can largely forget about cleanup.  If you want be polite and give up these resources early (this would be important if you wanted to launch new workers, or if you were using a large fraction of the cluster), you can tell them to stop immediately after completion of the last job by setting their timeout to zero.

```{r}
rrq_worker_message("TIMEOUT_SET" = 0)
```

This is common enough that we provide a helper function in hipercow:

```{r}
hipercow_rrq_stop_workers_once_idle()
```

which is hopefully self-explanatory.

# General considerations

## Permanence

You should not treat data in a submitted task as permanent; it is subject for deletion at any point!  So your aim should be to pull the data out of rrq as soon as you can.  Practically we won't delete data from the database for at least a few days after creation, but we make no guarantees.  We'll describe cleanup here later.

## Controlling the worker

The workers will use the `rrq` environment if it exists, failing that the `default` environment.  So if you need different packages and sources loaded on the workers on your normal tasks, you can do this by creating a different environment

```{r}
hipercow_environment_create("rrq", packages = "cowsay")
```

**TODO**: *work out how to refresh this environment; I think that's just a message to send*

You can submit your workers with any resources and parallel control you want (see `vignettes("parallel")` for details); pass these as `resources` and `parallel` to `hipercow_rrq_workers_submit()`.
